{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>RAG EVALUATION WITH RAGAS FRAMEWORK, LANGCHAIN, AZURE OPENAI</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Import all necessary dependencies here </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Azure OpenAI subscription details and keys are configured as environment variables </h4>\n",
    "<h4> Define embedding model and Vector store (In-memory) </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venvs\\ag_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "        azure_deployment='gpt-4o',\n",
    "    )\n",
    "embed_model = HuggingFaceEmbeddings(model_name=os.getenv('EMBEDDING_MODEL_PATH'))\n",
    "vector_store = InMemoryVectorStore(embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Testing the LLM chat API</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 31, 'total_tokens': 35, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f3927aa00d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-88adaa8e-f308-4c31-a65d-d5d1f9927b2c-0', usage_metadata={'input_tokens': 31, 'output_tokens': 4, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>From the master datasource, create the documents for indexing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = [\n",
    "    \"Andrew Ng is the CEO of Landing AI and is known for his pioneering work in deep learning. He is also widely recognized for democratizing AI education through platforms like Coursera.\",\n",
    "    \"Sam Altman is the CEO of OpenAI and has played a key role in advancing AI research and development. He is a strong advocate for creating safe and beneficial AI technologies.\",\n",
    "    \"Demis Hassabis is the CEO of DeepMind and is celebrated for his innovative approach to artificial intelligence. He gained prominence for developing systems that can master complex games like AlphaGo.\",\n",
    "    \"Sundar Pichai is the CEO of Google and Alphabet Inc., and he is praised for leading innovation across Google's vast product ecosystem. His leadership has significantly enhanced user experiences on a global scale.\",\n",
    "    \"Arvind Krishna is the CEO of IBM and is recognized for transforming the company towards cloud computing and AI solutions. He focuses on providing cutting-edge technologies to address modern business challenges.\",\n",
    "]\n",
    "\n",
    "langchain_documents = []\n",
    "\n",
    "for content in content_list:\n",
    "    langchain_documents.append(\n",
    "        Document(\n",
    "            page_content=content,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Store the input documents into Vector Store and then create retriever component from it</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(langchain_documents)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Define RAG retriever chain</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(relevant_docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "#query = \"Who is the CEO of OpenAI?\"\n",
    "\n",
    "#relevant_docs = retriever.invoke(query)\n",
    "#qa_chain.invoke({\"context\": format_docs(relevant_docs), \"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sample Validation data in the form of Questions and their expected responses</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = [\n",
    "    \"Which CEO is widely recognized for democratizing AI education through platforms like Coursera?\",\n",
    "    \"Who is Sam Altman?\",\n",
    "    \"Who is Demis Hassabis and how did he gained prominence?\",\n",
    "    \"Who is the CEO of Google and Alphabet Inc., praised for leading innovation across Google's product ecosystem?\",\n",
    "    \"How did Arvind Krishna transformed IBM?\",\n",
    "]\n",
    "\n",
    "expected_responses = [\n",
    "    \"Andrew Ng is the CEO of Landing AI and is widely recognized for democratizing AI education through platforms like Coursera.\",\n",
    "    \"Sam Altman is the CEO of OpenAI and has played a key role in advancing AI research and development. He strongly advocates for creating safe and beneficial AI technologies.\",\n",
    "    \"Demis Hassabis is the CEO of DeepMind and is celebrated for his innovative approach to artificial intelligence. He gained prominence for developing systems like AlphaGo that can master complex games.\",\n",
    "    \"Sundar Pichai is the CEO of Google and Alphabet Inc., praised for leading innovation across Google's vast product ecosystem. His leadership has significantly enhanced user experiences globally.\",\n",
    "    \"Arvind Krishna is the CEO of IBM and has transformed the company towards cloud computing and AI solutions. He focuses on delivering cutting-edge technologies to address modern business challenges.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generation of Evaluation dataset using sample validation data created above, for RAGAS</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for query, reference in zip(sample_queries, expected_responses):\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    response = qa_chain.invoke({\"context\": format_docs(relevant_docs), \"query\": query})\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": query,\n",
    "            \"retrieved_contexts\": [rdoc.page_content for rdoc in relevant_docs],\n",
    "            \"response\": response,\n",
    "            \"reference\": reference,\n",
    "        }\n",
    "    )\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Evaluate RAG retriever component with evaluation dataset prepared using RAGAS evaluation.</h4>\n",
    "<h4>Metrics evaluated against are - Faithfulness, Context Recall, Factual Correctness</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:30<00:00,  2.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 1.0000, 'context_recall': 1.0000, 'factual_correctness': 0.8860}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "evaluator_embed = LangchainEmbeddingsWrapper(embed_model)\n",
    "\n",
    "my_run_config = RunConfig(max_workers=5, timeout=60, max_retries=3)\n",
    "\n",
    "class TestCallback(BaseCallbackHandler):\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        pass\n",
    "        #print(f\"**********Prompts*********:\\n {prompts[0]}\\n\\n\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        pass\n",
    "        #print(f\"**********Response**********:\\n {response}\\n\\n\")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[Faithfulness(), LLMContextRecall(), FactualCorrectness()], #\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embed,\n",
    "    run_config=my_run_config,\n",
    "    raise_exceptions=True,\n",
    "    callbacks=[TestCallback()],\n",
    "    #batch_size=2,\n",
    ")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
